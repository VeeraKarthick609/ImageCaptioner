{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a2eee0",
   "metadata": {},
   "source": [
    "- This file contains the code for creating the encoder and decoder -- ViT and GPT2\n",
    "- The weights are copied from the pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65638658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# GPT2 config\n",
    "# the number of layers is 12\n",
    "# the embedding size is 768\n",
    "\n",
    "# to explore the keys and the values of the weights\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M\n",
    "\n",
    "# get the state dicts\n",
    "model_state_dict = gpt2.state_dict()\n",
    "\n",
    "# print them \n",
    "for k, v in model_state_dict.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dceee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257  # GPT-2 vocab size\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gelu(self.c_fc(x))\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()  # <-- FIX: Added super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # Q, K, V projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # attention (materializes the large (T,T) matrix for all queries and keys)\n",
    "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        attn = attn.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        y = attn @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        \n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()  # <-- FIX: Added super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # --- REFINEMENT: Weight Tying ---\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # <-- FIX: Added the entire forward method -->\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "\n",
    "        # forward the token and pos embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        # if we are given some desired targets, also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (B,T)) and complete\n",
    "        the sequence B times, each of length max_new_tokens.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long, crop it\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d5fe6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "def load_weights_from_hf(my_model: GPT, hf_model: GPT2LMHeadModel):\n",
    "    \"\"\"\n",
    "    Loads weights from a Hugging Face GPT2LMHeadModel into our custom GPT model.\n",
    "    \"\"\"\n",
    "    my_sd = my_model.state_dict()\n",
    "    hf_sd = hf_model.state_dict()\n",
    "\n",
    "    # The weights in Hugging Face's conv1d are transposed compared to nn.Linear\n",
    "    # We need to transpose them back\n",
    "    transposed_keys = ['c_attn.weight', 'c_proj.weight', 'c_fc.weight']\n",
    "\n",
    "    print(\"Copying weights...\")\n",
    "    for key in my_sd:\n",
    "        # The Hugging Face model has a \"transformer.\" prefix for its layers,\n",
    "        # and our lm_head is not inside the transformer block.\n",
    "        if key.startswith(\"transformer.\"):\n",
    "            hf_key = key\n",
    "        else: # it's the lm_head\n",
    "            hf_key = \"transformer.\" + key\n",
    "\n",
    "        # Special case for the lm_head, which is outside the 'transformer' block in HF model\n",
    "        if 'lm_head.weight' in key:\n",
    "            hf_key = 'lm_head.weight'\n",
    "            \n",
    "        if hf_key not in hf_sd:\n",
    "            print(f\"Skipping {key}, not found in Hugging Face model.\")\n",
    "            continue\n",
    "\n",
    "        # Check if this weight needs to be transposed\n",
    "        needs_transpose = any(tk in key for tk in transposed_keys)\n",
    "        \n",
    "        if needs_transpose:\n",
    "            print(f\"Copying and transposing: {key} <-- {hf_key}\")\n",
    "            my_sd[key].copy_(hf_sd[hf_key].T)\n",
    "        else:\n",
    "            print(f\"Copying directly:      {key} <-- {hf_key}\")\n",
    "            my_sd[key].copy_(hf_sd[hf_key])\n",
    "            \n",
    "    # Load the modified state dict into our model\n",
    "    my_model.load_state_dict(my_sd)\n",
    "    print(\"Weight copy complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5af003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up models and tokenizer...\n",
      "Copying weights...\n",
      "Copying directly:      transformer.wte.weight <-- transformer.wte.weight\n",
      "Copying directly:      transformer.wpe.weight <-- transformer.wpe.weight\n",
      "Copying directly:      transformer.h.0.ln_1.weight <-- transformer.h.0.ln_1.weight\n",
      "Copying directly:      transformer.h.0.ln_1.bias <-- transformer.h.0.ln_1.bias\n",
      "Skipping transformer.h.0.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.0.attn.c_attn.weight <-- transformer.h.0.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.0.attn.c_attn.bias <-- transformer.h.0.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.0.attn.c_proj.weight <-- transformer.h.0.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.0.attn.c_proj.bias <-- transformer.h.0.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.0.ln_2.weight <-- transformer.h.0.ln_2.weight\n",
      "Copying directly:      transformer.h.0.ln_2.bias <-- transformer.h.0.ln_2.bias\n",
      "Copying and transposing: transformer.h.0.mlp.c_fc.weight <-- transformer.h.0.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.0.mlp.c_fc.bias <-- transformer.h.0.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.0.mlp.c_proj.weight <-- transformer.h.0.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.0.mlp.c_proj.bias <-- transformer.h.0.mlp.c_proj.bias\n",
      "Copying directly:      transformer.h.1.ln_1.weight <-- transformer.h.1.ln_1.weight\n",
      "Copying directly:      transformer.h.1.ln_1.bias <-- transformer.h.1.ln_1.bias\n",
      "Skipping transformer.h.1.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.1.attn.c_attn.weight <-- transformer.h.1.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.1.attn.c_attn.bias <-- transformer.h.1.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.1.attn.c_proj.weight <-- transformer.h.1.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.1.attn.c_proj.bias <-- transformer.h.1.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.1.ln_2.weight <-- transformer.h.1.ln_2.weight\n",
      "Copying directly:      transformer.h.1.ln_2.bias <-- transformer.h.1.ln_2.bias\n",
      "Copying and transposing: transformer.h.1.mlp.c_fc.weight <-- transformer.h.1.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.1.mlp.c_fc.bias <-- transformer.h.1.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.1.mlp.c_proj.weight <-- transformer.h.1.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.1.mlp.c_proj.bias <-- transformer.h.1.mlp.c_proj.bias\n",
      "Copying directly:      transformer.h.2.ln_1.weight <-- transformer.h.2.ln_1.weight\n",
      "Copying directly:      transformer.h.2.ln_1.bias <-- transformer.h.2.ln_1.bias\n",
      "Skipping transformer.h.2.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.2.attn.c_attn.weight <-- transformer.h.2.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.2.attn.c_attn.bias <-- transformer.h.2.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.2.attn.c_proj.weight <-- transformer.h.2.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.2.attn.c_proj.bias <-- transformer.h.2.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.2.ln_2.weight <-- transformer.h.2.ln_2.weight\n",
      "Copying directly:      transformer.h.2.ln_2.bias <-- transformer.h.2.ln_2.bias\n",
      "Copying and transposing: transformer.h.2.mlp.c_fc.weight <-- transformer.h.2.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.2.mlp.c_fc.bias <-- transformer.h.2.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.2.mlp.c_proj.weight <-- transformer.h.2.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.2.mlp.c_proj.bias <-- transformer.h.2.mlp.c_proj.bias\n",
      "Copying directly:      transformer.h.3.ln_1.weight <-- transformer.h.3.ln_1.weight\n",
      "Copying directly:      transformer.h.3.ln_1.bias <-- transformer.h.3.ln_1.bias\n",
      "Skipping transformer.h.3.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.3.attn.c_attn.weight <-- transformer.h.3.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.3.attn.c_attn.bias <-- transformer.h.3.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.3.attn.c_proj.weight <-- transformer.h.3.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.3.attn.c_proj.bias <-- transformer.h.3.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.3.ln_2.weight <-- transformer.h.3.ln_2.weight\n",
      "Copying directly:      transformer.h.3.ln_2.bias <-- transformer.h.3.ln_2.bias\n",
      "Copying and transposing: transformer.h.3.mlp.c_fc.weight <-- transformer.h.3.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.3.mlp.c_fc.bias <-- transformer.h.3.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.3.mlp.c_proj.weight <-- transformer.h.3.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.3.mlp.c_proj.bias <-- transformer.h.3.mlp.c_proj.bias\n",
      "Copying directly:      transformer.h.4.ln_1.weight <-- transformer.h.4.ln_1.weight\n",
      "Copying directly:      transformer.h.4.ln_1.bias <-- transformer.h.4.ln_1.bias\n",
      "Skipping transformer.h.4.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.4.attn.c_attn.weight <-- transformer.h.4.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.4.attn.c_attn.bias <-- transformer.h.4.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.4.attn.c_proj.weight <-- transformer.h.4.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.4.attn.c_proj.bias <-- transformer.h.4.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.4.ln_2.weight <-- transformer.h.4.ln_2.weight\n",
      "Copying directly:      transformer.h.4.ln_2.bias <-- transformer.h.4.ln_2.bias\n",
      "Copying and transposing: transformer.h.4.mlp.c_fc.weight <-- transformer.h.4.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.4.mlp.c_fc.bias <-- transformer.h.4.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.4.mlp.c_proj.weight <-- transformer.h.4.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.4.mlp.c_proj.bias <-- transformer.h.4.mlp.c_proj.bias\n",
      "Copying directly:      transformer.h.5.ln_1.weight <-- transformer.h.5.ln_1.weight\n",
      "Copying directly:      transformer.h.5.ln_1.bias <-- transformer.h.5.ln_1.bias\n",
      "Skipping transformer.h.5.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.5.attn.c_attn.weight <-- transformer.h.5.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.5.attn.c_attn.bias <-- transformer.h.5.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.5.attn.c_proj.weight <-- transformer.h.5.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.5.attn.c_proj.bias <-- transformer.h.5.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.5.ln_2.weight <-- transformer.h.5.ln_2.weight\n",
      "Copying directly:      transformer.h.5.ln_2.bias <-- transformer.h.5.ln_2.bias\n",
      "Copying and transposing: transformer.h.5.mlp.c_fc.weight <-- transformer.h.5.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.5.mlp.c_fc.bias <-- transformer.h.5.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.5.mlp.c_proj.weight <-- transformer.h.5.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.5.mlp.c_proj.bias <-- transformer.h.5.mlp.c_proj.bias\n",
      "Copying directly:      transformer.h.6.ln_1.weight <-- transformer.h.6.ln_1.weight\n",
      "Copying directly:      transformer.h.6.ln_1.bias <-- transformer.h.6.ln_1.bias\n",
      "Skipping transformer.h.6.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.6.attn.c_attn.weight <-- transformer.h.6.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.6.attn.c_attn.bias <-- transformer.h.6.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.6.attn.c_proj.weight <-- transformer.h.6.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.6.attn.c_proj.bias <-- transformer.h.6.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.6.ln_2.weight <-- transformer.h.6.ln_2.weight\n",
      "Copying directly:      transformer.h.6.ln_2.bias <-- transformer.h.6.ln_2.bias\n",
      "Copying and transposing: transformer.h.6.mlp.c_fc.weight <-- transformer.h.6.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.6.mlp.c_fc.bias <-- transformer.h.6.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.6.mlp.c_proj.weight <-- transformer.h.6.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.6.mlp.c_proj.bias <-- transformer.h.6.mlp.c_proj.bias\n",
      "Copying directly:      transformer.h.7.ln_1.weight <-- transformer.h.7.ln_1.weight\n",
      "Copying directly:      transformer.h.7.ln_1.bias <-- transformer.h.7.ln_1.bias\n",
      "Skipping transformer.h.7.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.7.attn.c_attn.weight <-- transformer.h.7.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.7.attn.c_attn.bias <-- transformer.h.7.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.7.attn.c_proj.weight <-- transformer.h.7.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.7.attn.c_proj.bias <-- transformer.h.7.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.7.ln_2.weight <-- transformer.h.7.ln_2.weight\n",
      "Copying directly:      transformer.h.7.ln_2.bias <-- transformer.h.7.ln_2.bias\n",
      "Copying and transposing: transformer.h.7.mlp.c_fc.weight <-- transformer.h.7.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.7.mlp.c_fc.bias <-- transformer.h.7.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.7.mlp.c_proj.weight <-- transformer.h.7.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.7.mlp.c_proj.bias <-- transformer.h.7.mlp.c_proj.bias\n",
      "Copying directly:      transformer.h.8.ln_1.weight <-- transformer.h.8.ln_1.weight\n",
      "Copying directly:      transformer.h.8.ln_1.bias <-- transformer.h.8.ln_1.bias\n",
      "Skipping transformer.h.8.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.8.attn.c_attn.weight <-- transformer.h.8.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.8.attn.c_attn.bias <-- transformer.h.8.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.8.attn.c_proj.weight <-- transformer.h.8.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.8.attn.c_proj.bias <-- transformer.h.8.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.8.ln_2.weight <-- transformer.h.8.ln_2.weight\n",
      "Copying directly:      transformer.h.8.ln_2.bias <-- transformer.h.8.ln_2.bias\n",
      "Copying and transposing: transformer.h.8.mlp.c_fc.weight <-- transformer.h.8.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.8.mlp.c_fc.bias <-- transformer.h.8.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.8.mlp.c_proj.weight <-- transformer.h.8.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.8.mlp.c_proj.bias <-- transformer.h.8.mlp.c_proj.bias\n",
      "Copying directly:      transformer.h.9.ln_1.weight <-- transformer.h.9.ln_1.weight\n",
      "Copying directly:      transformer.h.9.ln_1.bias <-- transformer.h.9.ln_1.bias\n",
      "Skipping transformer.h.9.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.9.attn.c_attn.weight <-- transformer.h.9.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.9.attn.c_attn.bias <-- transformer.h.9.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.9.attn.c_proj.weight <-- transformer.h.9.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.9.attn.c_proj.bias <-- transformer.h.9.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.9.ln_2.weight <-- transformer.h.9.ln_2.weight\n",
      "Copying directly:      transformer.h.9.ln_2.bias <-- transformer.h.9.ln_2.bias\n",
      "Copying and transposing: transformer.h.9.mlp.c_fc.weight <-- transformer.h.9.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.9.mlp.c_fc.bias <-- transformer.h.9.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.9.mlp.c_proj.weight <-- transformer.h.9.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.9.mlp.c_proj.bias <-- transformer.h.9.mlp.c_proj.bias\n",
      "Copying directly:      transformer.h.10.ln_1.weight <-- transformer.h.10.ln_1.weight\n",
      "Copying directly:      transformer.h.10.ln_1.bias <-- transformer.h.10.ln_1.bias\n",
      "Skipping transformer.h.10.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.10.attn.c_attn.weight <-- transformer.h.10.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.10.attn.c_attn.bias <-- transformer.h.10.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.10.attn.c_proj.weight <-- transformer.h.10.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.10.attn.c_proj.bias <-- transformer.h.10.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.10.ln_2.weight <-- transformer.h.10.ln_2.weight\n",
      "Copying directly:      transformer.h.10.ln_2.bias <-- transformer.h.10.ln_2.bias\n",
      "Copying and transposing: transformer.h.10.mlp.c_fc.weight <-- transformer.h.10.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.10.mlp.c_fc.bias <-- transformer.h.10.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.10.mlp.c_proj.weight <-- transformer.h.10.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.10.mlp.c_proj.bias <-- transformer.h.10.mlp.c_proj.bias\n",
      "Copying directly:      transformer.h.11.ln_1.weight <-- transformer.h.11.ln_1.weight\n",
      "Copying directly:      transformer.h.11.ln_1.bias <-- transformer.h.11.ln_1.bias\n",
      "Skipping transformer.h.11.attn.bias, not found in Hugging Face model.\n",
      "Copying and transposing: transformer.h.11.attn.c_attn.weight <-- transformer.h.11.attn.c_attn.weight\n",
      "Copying directly:      transformer.h.11.attn.c_attn.bias <-- transformer.h.11.attn.c_attn.bias\n",
      "Copying and transposing: transformer.h.11.attn.c_proj.weight <-- transformer.h.11.attn.c_proj.weight\n",
      "Copying directly:      transformer.h.11.attn.c_proj.bias <-- transformer.h.11.attn.c_proj.bias\n",
      "Copying directly:      transformer.h.11.ln_2.weight <-- transformer.h.11.ln_2.weight\n",
      "Copying directly:      transformer.h.11.ln_2.bias <-- transformer.h.11.ln_2.bias\n",
      "Copying and transposing: transformer.h.11.mlp.c_fc.weight <-- transformer.h.11.mlp.c_fc.weight\n",
      "Copying directly:      transformer.h.11.mlp.c_fc.bias <-- transformer.h.11.mlp.c_fc.bias\n",
      "Copying and transposing: transformer.h.11.mlp.c_proj.weight <-- transformer.h.11.mlp.c_proj.weight\n",
      "Copying directly:      transformer.h.11.mlp.c_proj.bias <-- transformer.h.11.mlp.c_proj.bias\n",
      "Copying directly:      transformer.ln_f.weight <-- transformer.ln_f.weight\n",
      "Copying directly:      transformer.ln_f.bias <-- transformer.ln_f.bias\n",
      "Copying directly:      lm_head.weight <-- lm_head.weight\n",
      "Weight copy complete.\n",
      "\n",
      "--- Generating Text with Your Custom GPT Model ---\n",
      "Prompt: 'Hello, I am a language model,'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "--------------------------------------------------\n",
      "Hello, I am a language model, not a concept. The only thing I have to worry about in the future are your ideas and when to keep changing it will not work out. I am ready for you to work for it.\"\n",
      "\n",
      "A few days later, on April 20,\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Generating Text with Hugging Face Model for Comparison ---\n",
      "\n",
      "Generated Text (Hugging Face):\n",
      "--------------------------------------------------\n",
      "Hello, I am a language model, I've been learning it for some time and I think that's where it all started and it's what's really exciting to me is my language and I'm happy for that. I'd love to share more and you see how I'll approach it\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# main_demo.py\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- 1. Setup Models and Tokenizer ---\n",
    "    print(\"Setting up models and tokenizer...\")\n",
    "    \n",
    "    # Instantiate our custom model\n",
    "    config = GPTConfig()\n",
    "    my_gpt = GPT(config)\n",
    "    my_gpt.eval() # Set to evaluation mode\n",
    "\n",
    "    # Load the official Hugging Face model and tokenizer\n",
    "    hf_model_name = 'gpt2'\n",
    "    hf_gpt = GPT2LMHeadModel.from_pretrained(hf_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "\n",
    "    # --- 2. Copy Weights ---\n",
    "    load_weights_from_hf(my_gpt, hf_gpt)\n",
    "\n",
    "    # --- 3. Generate Tokens ---\n",
    "    print(\"\\n--- Generating Text with Your Custom GPT Model ---\")\n",
    "    prompt = \"Hello, I am a language model,\"\n",
    "    \n",
    "    # Encode the prompt into token IDs\n",
    "    start_ids = tokenizer.encode(prompt, return_tensors='pt') # Get PyTorch tensors\n",
    "    \n",
    "    # Generate text\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    generated_ids = my_gpt.generate(\n",
    "        idx=start_ids, \n",
    "        max_new_tokens=50, \n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    # Decode the generated token IDs back to a string\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"\\nGenerated Text:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(generated_text)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # --- Verification (Optional): Generate with HF model to compare ---\n",
    "    print(\"\\n--- Generating Text with Hugging Face Model for Comparison ---\")\n",
    "    hf_generated_ids = hf_gpt.generate(\n",
    "        start_ids,\n",
    "        max_length=len(start_ids[0]) + 50,\n",
    "        top_k=50,\n",
    "        do_sample=True # Important to make it stochastic like ours\n",
    "    )\n",
    "    hf_generated_text = tokenizer.decode(hf_generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"\\nGenerated Text (Hugging Face):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(hf_generated_text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae1561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
