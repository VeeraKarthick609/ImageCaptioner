{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd279e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ImageCaptioner\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:__main__:Loading model from ..\\vit-gpt2-coco-finetuned-from-scratch\n",
      "INFO:__main__:Using device: cuda\n",
      "INFO:__main__:Loading model with manual configuration...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Image Captioning Inference Script\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Loaded trained model weights\n",
      "INFO:__main__:Model loaded and ready for inference\n",
      "INFO:__main__:Testing with sample COCO images:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Image 1: http://images.cocodataset.org/val2017/000000039769.jpg\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'max_length': 64, 'do_sample': True, 'temperature': 0.7, 'top_p': 0.9, 'decoder_start_token_id': 50256}. If this is not desired, please set these values explicitly.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. You should pass an instance of `Cache` instead, e.g. `past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'max_length': 64, 'decoder_start_token_id': 50256}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Caption: cat on bed to a on laptop a and t bear\n",
      "Sampling Caption: cat on bed front a and mouse\n",
      "------------------------------------------------------------\n",
      "\n",
      "Image 2: http://images.cocodataset.org/val2017/000000397133.jpg\n",
      "----------------------------------------\n",
      "Greedy Caption: woman on kitchen in with and oven\n",
      "Sampling Caption: woman at table several in kitchen and.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Image 3: http://images.cocodataset.org/val2017/000000037777.jpg\n",
      "----------------------------------------\n",
      "Greedy Caption: kitchen a and stove a and oven\n",
      "Sampling Caption: kitchen a and a with cabinets a and stove\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¯ Interactive Mode\n",
      "Enter image paths or URLs to caption (or 'quit' to exit)\n",
      "\n",
      "You can test with:\n",
      "â€¢ Local image paths: /path/to/image.jpg\n",
      "â€¢ URLs: http://images.cocodataset.org/val2017/000000039769.jpg\n",
      "\n",
      "Generation options:\n",
      "â€¢ Add 'greedy' for deterministic mode: image.jpg greedy\n",
      "â€¢ Default: sampling mode (more creative)\n",
      "Please enter a valid path or URL\n",
      "\n",
      "Thanks for using the Image Captioning Script!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from PIL import Image\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ImageCaptioner:\n",
    "    def __init__(self, model_path, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the Image Captioner\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to the saved model directory\n",
    "            device (str, optional): Device to run inference on. Defaults to auto-detect.\n",
    "        \"\"\"\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_path = Path(model_path)\n",
    "        \n",
    "        # Validate model path\n",
    "        if not self.model_path.exists():\n",
    "            raise FileNotFoundError(f\"Model path does not exist: {model_path}\")\n",
    "        \n",
    "        # Load the model components\n",
    "        self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the trained model, tokenizer, and image processor\"\"\"\n",
    "        logger.info(f\"Loading model from {self.model_path}\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        try:\n",
    "            # Method 1: Try to load complete fine-tuned model\n",
    "            if (self.model_path / \"config.json\").exists():\n",
    "                logger.info(\"Loading complete fine-tuned model...\")\n",
    "                self.model = VisionEncoderDecoderModel.from_pretrained(self.model_path)\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "                self.image_processor = ViTImageProcessor.from_pretrained(self.model_path)\n",
    "            else:\n",
    "                # Method 2: Fallback to manual loading (your original approach, but improved)\n",
    "                logger.info(\"Loading model with manual configuration...\")\n",
    "                self._load_model_manual()\n",
    "            \n",
    "            # Set up generation configuration properly\n",
    "            self._setup_generation_config()\n",
    "            \n",
    "            # Move model to device and set to evaluation mode\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            logger.info(\"Model loaded and ready for inference\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _load_model_manual(self):\n",
    "        \"\"\"Manual model loading as fallback\"\"\"\n",
    "        # Load components\n",
    "        encoder_model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "        decoder_model_name = \"gpt2\"\n",
    "        \n",
    "        # Try to load saved components first, fallback to base models\n",
    "        try:\n",
    "            self.image_processor = ViTImageProcessor.from_pretrained(self.model_path)\n",
    "        except:\n",
    "            self.image_processor = ViTImageProcessor.from_pretrained(encoder_model_name)\n",
    "            logger.warning(\"Using base ViT image processor\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        except:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\n",
    "            # Ensure pad token is set\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            logger.warning(\"Using base GPT2 tokenizer\")\n",
    "        \n",
    "        # Create model architecture\n",
    "        self.model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "            encoder_model_name, decoder_model_name\n",
    "        )\n",
    "        \n",
    "        # Try to load trained weights\n",
    "        model_state_path = self.model_path / \"best_model.pth\"\n",
    "        if model_state_path.exists():\n",
    "            try:\n",
    "                state_dict = torch.load(model_state_path, map_location=self.device)\n",
    "                # Handle potential key mismatches\n",
    "                missing_keys, unexpected_keys = self.model.load_state_dict(state_dict, strict=False)\n",
    "                if missing_keys:\n",
    "                    logger.warning(f\"Missing keys in state dict: {len(missing_keys)} keys\")\n",
    "                if unexpected_keys:\n",
    "                    logger.warning(f\"Unexpected keys in state dict: {len(unexpected_keys)} keys\")\n",
    "                logger.info(\"Loaded trained model weights\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load trained weights: {str(e)}\")\n",
    "                logger.warning(\"Using base model weights\")\n",
    "        else:\n",
    "            logger.warning(\"No trained weights found, using base model\")\n",
    "    \n",
    "    def _setup_generation_config(self):\n",
    "        \"\"\"Set up generation configuration properly\"\"\"\n",
    "        # GPT2 doesn't support beam search in VisionEncoderDecoder, so use greedy/sampling\n",
    "        generation_config = GenerationConfig(\n",
    "            decoder_start_token_id=self.tokenizer.bos_token_id,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            max_length=64,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_beams=1,  # Must be 1 for GPT2 decoder\n",
    "            do_sample=True,  # Use sampling since beam search not supported\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        \n",
    "        # Save the generation config to the model\n",
    "        self.model.generation_config = generation_config\n",
    "    \n",
    "    def caption_image(self, image_input, max_new_tokens=64, temperature=0.7, \n",
    "                     top_p=0.9, do_sample=True):\n",
    "        \"\"\"\n",
    "        Generate a caption for a single image\n",
    "        \n",
    "        Args:\n",
    "            image_input (str): Path to image file or URL\n",
    "            max_new_tokens (int): Maximum number of new tokens to generate\n",
    "            temperature (float): Sampling temperature\n",
    "            top_p (float): Nucleus sampling parameter\n",
    "            do_sample (bool): Whether to use sampling (True) or greedy (False)\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated caption\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input\n",
    "            if not image_input or not image_input.strip():\n",
    "                raise ValueError(\"Empty image input\")\n",
    "            \n",
    "            # Load image from URL or local path\n",
    "            image = self._load_image(image_input)\n",
    "            \n",
    "            # Preprocess the image\n",
    "            pixel_values = self.image_processor(\n",
    "                image, \n",
    "                return_tensors=\"pt\"\n",
    "            ).pixel_values.to(self.device)\n",
    "            \n",
    "            # Create generation config for this inference\n",
    "            # NOTE: GPT2 doesn't support beam search in VisionEncoderDecoder\n",
    "            generation_config = GenerationConfig(\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=do_sample,\n",
    "                temperature=temperature if do_sample else 1.0,\n",
    "                top_p=top_p if do_sample else 1.0,\n",
    "                num_beams=1,  # Must be 1 for GPT2 decoder\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=3,\n",
    "            )\n",
    "            \n",
    "            # Generate caption\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    pixel_values, \n",
    "                    generation_config=generation_config\n",
    "                )\n",
    "            \n",
    "            # Decode the generated tokens\n",
    "            caption = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up GPU memory if using CUDA\n",
    "            if self.device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            return caption.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing image {image_input}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_image(self, image_input):\n",
    "        \"\"\"Load image from URL or local path\"\"\"\n",
    "        if image_input.startswith(('http://', 'https://')):\n",
    "            try:\n",
    "                response = requests.get(image_input, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            except requests.RequestException as e:\n",
    "                raise RuntimeError(f\"Failed to download image from URL: {str(e)}\")\n",
    "        else:\n",
    "            image_path = Path(image_input)\n",
    "            if not image_path.exists():\n",
    "                raise FileNotFoundError(f\"Image file not found: {image_input}\")\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to open image file: {str(e)}\")\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def caption_batch(self, image_inputs, **kwargs):\n",
    "        \"\"\"Generate captions for a batch of images\"\"\"\n",
    "        results = []\n",
    "        for image_input in image_inputs:\n",
    "            caption = self.caption_image(image_input, **kwargs)\n",
    "            results.append({\n",
    "                'image': image_input,\n",
    "                'caption': caption,\n",
    "                'success': caption is not None\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "def test_sample_images(captioner):\n",
    "    \"\"\"Test captioning on sample COCO images\"\"\"\n",
    "    \n",
    "    # Sample COCO image URLs\n",
    "    sample_urls = [\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "        \"http://images.cocodataset.org/val2017/000000397133.jpg\", \n",
    "        \"http://images.cocodataset.org/val2017/000000037777.jpg\"\n",
    "    ]\n",
    "    \n",
    "    logger.info(\"Testing with sample COCO images:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, url in enumerate(sample_urls, 1):\n",
    "        print(f\"\\nImage {i}: {url}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Test both greedy and sampling\n",
    "        caption_greedy = captioner.caption_image(url, do_sample=False)\n",
    "        caption_sample = captioner.caption_image(url, do_sample=True, temperature=0.7)\n",
    "        \n",
    "        if caption_greedy:\n",
    "            print(f\"Greedy Caption: {caption_greedy}\")\n",
    "        if caption_sample:\n",
    "            print(f\"Sampling Caption: {caption_sample}\")\n",
    "        \n",
    "        if not caption_greedy and not caption_sample:\n",
    "            print(\"Failed to generate caption\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run image captioning inference\"\"\"\n",
    "    \n",
    "    # Configuration - UPDATE THIS PATH\n",
    "    MODEL_PATH = \"../vit-gpt2-coco-finetuned-from-scratch\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Image Captioning Inference Script\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Initialize the captioner\n",
    "        captioner = ImageCaptioner(MODEL_PATH)\n",
    "        \n",
    "        # Test with sample images\n",
    "        test_sample_images(captioner)\n",
    "        \n",
    "        # Interactive mode\n",
    "        print(\"\\nðŸŽ¯ Interactive Mode\")\n",
    "        print(\"Enter image paths or URLs to caption (or 'quit' to exit)\")\n",
    "        print(\"\\nYou can test with:\")\n",
    "        print(\"â€¢ Local image paths: /path/to/image.jpg\")\n",
    "        print(\"â€¢ URLs: http://images.cocodataset.org/val2017/000000039769.jpg\")\n",
    "        print(\"\\nGeneration options:\")\n",
    "        print(\"â€¢ Add 'greedy' for deterministic mode: image.jpg greedy\")\n",
    "        print(\"â€¢ Default: sampling mode (more creative)\")\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"\\nEnter image path or URL: \").strip()\n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "            \n",
    "            if user_input:\n",
    "                # Parse input for generation mode\n",
    "                parts = user_input.split()\n",
    "                image_path = parts[0]\n",
    "                use_greedy = len(parts) > 1 and 'greedy' in parts[1].lower()\n",
    "                \n",
    "                caption = captioner.caption_image(\n",
    "                    image_path, \n",
    "                    do_sample=not use_greedy,\n",
    "                    temperature=0.8 if not use_greedy else 1.0,\n",
    "                    top_p=0.9 if not use_greedy else 1.0\n",
    "                )\n",
    "                \n",
    "                if caption:\n",
    "                    mode = \"Greedy\" if use_greedy else \"Sampling\"\n",
    "                    print(f\"Caption ({mode}): {caption}\")\n",
    "                else:\n",
    "                    print(\"Failed to generate caption\")\n",
    "            else:\n",
    "                print(\"Please enter a valid path or URL\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize captioner: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nThanks for using the Image Captioning Script!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c14879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
